{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dae55b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pages= 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6ce54d6ade1438489b2a6279ba2beb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/3.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hetal\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:123: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\hetal\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  \" https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\"\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'safe_open' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/flan-t5-large\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 19\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForSeq2SeqLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgoogle/flan-t5-large\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mquery_from_list\u001b[39m(query, options, tok_len):\n\u001b[0;32m     22\u001b[0m     t5query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mQuestion: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Context: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptions\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\"\"\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:463\u001b[0m, in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_auth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    460\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    461\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mFutureWarning\u001b[39;00m\n\u001b[0;32m    462\u001b[0m     )\n\u001b[1;32m--> 463\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    464\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    465\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`token` and `use_auth_token` are both specified. Please set only the argument `token`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    466\u001b[0m         )\n\u001b[0;32m    467\u001b[0m     token \u001b[38;5;241m=\u001b[39m use_auth_token\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\modeling_utils.py:2184\u001b[0m, in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2036\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m   2037\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_pretrained\u001b[39m(\n\u001b[0;32m   2038\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2049\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2050\u001b[0m ):\n\u001b[0;32m   2051\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2052\u001b[0m \u001b[38;5;124;03m    Instantiate a pretrained pytorch model from a pre-trained model configuration.\u001b[39;00m\n\u001b[0;32m   2053\u001b[0m \n\u001b[0;32m   2054\u001b[0m \u001b[38;5;124;03m    The model is set in evaluation mode by default using `model.eval()` (Dropout modules are deactivated). To train\u001b[39;00m\n\u001b[0;32m   2055\u001b[0m \u001b[38;5;124;03m    the model, you should first set it back in training mode with `model.train()`.\u001b[39;00m\n\u001b[0;32m   2056\u001b[0m \n\u001b[0;32m   2057\u001b[0m \u001b[38;5;124;03m    The warning *Weights from XXX not initialized from pretrained model* means that the weights of XXX do not come\u001b[39;00m\n\u001b[0;32m   2058\u001b[0m \u001b[38;5;124;03m    pretrained with the rest of the model. It is up to you to train those weights with a downstream fine-tuning\u001b[39;00m\n\u001b[0;32m   2059\u001b[0m \u001b[38;5;124;03m    task.\u001b[39;00m\n\u001b[0;32m   2060\u001b[0m \n\u001b[0;32m   2061\u001b[0m \u001b[38;5;124;03m    The warning *Weights from XXX not used in YYY* means that the layer XXX is not used by YYY, therefore those\u001b[39;00m\n\u001b[0;32m   2062\u001b[0m \u001b[38;5;124;03m    weights are discarded.\u001b[39;00m\n\u001b[0;32m   2063\u001b[0m \n\u001b[0;32m   2064\u001b[0m \u001b[38;5;124;03m    Parameters:\u001b[39;00m\n\u001b[0;32m   2065\u001b[0m \u001b[38;5;124;03m        pretrained_model_name_or_path (`str` or `os.PathLike`, *optional*):\u001b[39;00m\n\u001b[0;32m   2066\u001b[0m \u001b[38;5;124;03m            Can be either:\u001b[39;00m\n\u001b[0;32m   2067\u001b[0m \n\u001b[0;32m   2068\u001b[0m \u001b[38;5;124;03m                - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co.\u001b[39;00m\n\u001b[0;32m   2069\u001b[0m \u001b[38;5;124;03m                  Valid model ids can be located at the root-level, like `bert-base-uncased`, or namespaced under a\u001b[39;00m\n\u001b[0;32m   2070\u001b[0m \u001b[38;5;124;03m                  user or organization name, like `dbmdz/bert-base-german-cased`.\u001b[39;00m\n\u001b[0;32m   2071\u001b[0m \u001b[38;5;124;03m                - A path to a *directory* containing model weights saved using\u001b[39;00m\n\u001b[0;32m   2072\u001b[0m \u001b[38;5;124;03m                  [`~PreTrainedModel.save_pretrained`], e.g., `./my_model_directory/`.\u001b[39;00m\n\u001b[0;32m   2073\u001b[0m \u001b[38;5;124;03m                - A path or url to a *tensorflow index checkpoint file* (e.g, `./tf_model/model.ckpt.index`). In\u001b[39;00m\n\u001b[0;32m   2074\u001b[0m \u001b[38;5;124;03m                  this case, `from_tf` should be set to `True` and a configuration object should be provided as\u001b[39;00m\n\u001b[0;32m   2075\u001b[0m \u001b[38;5;124;03m                  `config` argument. This loading path is slower than converting the TensorFlow checkpoint in a\u001b[39;00m\n\u001b[0;32m   2076\u001b[0m \u001b[38;5;124;03m                  PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.\u001b[39;00m\n\u001b[0;32m   2077\u001b[0m \u001b[38;5;124;03m                - A path or url to a model folder containing a *flax checkpoint file* in *.msgpack* format (e.g,\u001b[39;00m\n\u001b[0;32m   2078\u001b[0m \u001b[38;5;124;03m                  `./flax_model/` containing `flax_model.msgpack`). In this case, `from_flax` should be set to\u001b[39;00m\n\u001b[0;32m   2079\u001b[0m \u001b[38;5;124;03m                  `True`.\u001b[39;00m\n\u001b[0;32m   2080\u001b[0m \u001b[38;5;124;03m                - `None` if you are both providing the configuration and state dictionary (resp. with keyword\u001b[39;00m\n\u001b[0;32m   2081\u001b[0m \u001b[38;5;124;03m                  arguments `config` and `state_dict`).\u001b[39;00m\n\u001b[0;32m   2082\u001b[0m \u001b[38;5;124;03m        model_args (sequence of positional arguments, *optional*):\u001b[39;00m\n\u001b[0;32m   2083\u001b[0m \u001b[38;5;124;03m            All remaining positional arguments will be passed to the underlying model's `__init__` method.\u001b[39;00m\n\u001b[0;32m   2084\u001b[0m \u001b[38;5;124;03m        config (`Union[PretrainedConfig, str, os.PathLike]`, *optional*):\u001b[39;00m\n\u001b[0;32m   2085\u001b[0m \u001b[38;5;124;03m            Can be either:\u001b[39;00m\n\u001b[0;32m   2086\u001b[0m \n\u001b[0;32m   2087\u001b[0m \u001b[38;5;124;03m                - an instance of a class derived from [`PretrainedConfig`],\u001b[39;00m\n\u001b[0;32m   2088\u001b[0m \u001b[38;5;124;03m                - a string or path valid as input to [`~PretrainedConfig.from_pretrained`].\u001b[39;00m\n\u001b[0;32m   2089\u001b[0m \n\u001b[0;32m   2090\u001b[0m \u001b[38;5;124;03m            Configuration for the model to use instead of an automatically loaded configuration. Configuration can\u001b[39;00m\n\u001b[0;32m   2091\u001b[0m \u001b[38;5;124;03m            be automatically loaded when:\u001b[39;00m\n\u001b[0;32m   2092\u001b[0m \n\u001b[0;32m   2093\u001b[0m \u001b[38;5;124;03m                - The model is a model provided by the library (loaded with the *model id* string of a pretrained\u001b[39;00m\n\u001b[0;32m   2094\u001b[0m \u001b[38;5;124;03m                  model).\u001b[39;00m\n\u001b[0;32m   2095\u001b[0m \u001b[38;5;124;03m                - The model was saved using [`~PreTrainedModel.save_pretrained`] and is reloaded by supplying the\u001b[39;00m\n\u001b[0;32m   2096\u001b[0m \u001b[38;5;124;03m                  save directory.\u001b[39;00m\n\u001b[0;32m   2097\u001b[0m \u001b[38;5;124;03m                - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a\u001b[39;00m\n\u001b[0;32m   2098\u001b[0m \u001b[38;5;124;03m                  configuration JSON file named *config.json* is found in the directory.\u001b[39;00m\n\u001b[0;32m   2099\u001b[0m \u001b[38;5;124;03m        state_dict (`Dict[str, torch.Tensor]`, *optional*):\u001b[39;00m\n\u001b[0;32m   2100\u001b[0m \u001b[38;5;124;03m            A state dictionary to use instead of a state dictionary loaded from saved weights file.\u001b[39;00m\n\u001b[0;32m   2101\u001b[0m \n\u001b[0;32m   2102\u001b[0m \u001b[38;5;124;03m            This option can be used if you want to create a model from a pretrained configuration but load your own\u001b[39;00m\n\u001b[0;32m   2103\u001b[0m \u001b[38;5;124;03m            weights. In this case though, you should check if using [`~PreTrainedModel.save_pretrained`] and\u001b[39;00m\n\u001b[0;32m   2104\u001b[0m \u001b[38;5;124;03m            [`~PreTrainedModel.from_pretrained`] is not a simpler option.\u001b[39;00m\n\u001b[0;32m   2105\u001b[0m \u001b[38;5;124;03m        cache_dir (`Union[str, os.PathLike]`, *optional*):\u001b[39;00m\n\u001b[0;32m   2106\u001b[0m \u001b[38;5;124;03m            Path to a directory in which a downloaded pretrained model configuration should be cached if the\u001b[39;00m\n\u001b[0;32m   2107\u001b[0m \u001b[38;5;124;03m            standard cache should not be used.\u001b[39;00m\n\u001b[0;32m   2108\u001b[0m \u001b[38;5;124;03m        from_tf (`bool`, *optional*, defaults to `False`):\u001b[39;00m\n\u001b[0;32m   2109\u001b[0m \u001b[38;5;124;03m            Load the model weights from a TensorFlow checkpoint save file (see docstring of\u001b[39;00m\n\u001b[0;32m   2110\u001b[0m \u001b[38;5;124;03m            `pretrained_model_name_or_path` argument).\u001b[39;00m\n\u001b[0;32m   2111\u001b[0m \u001b[38;5;124;03m        from_flax (`bool`, *optional*, defaults to `False`):\u001b[39;00m\n\u001b[0;32m   2112\u001b[0m \u001b[38;5;124;03m            Load the model weights from a Flax checkpoint save file (see docstring of\u001b[39;00m\n\u001b[0;32m   2113\u001b[0m \u001b[38;5;124;03m            `pretrained_model_name_or_path` argument).\u001b[39;00m\n\u001b[0;32m   2114\u001b[0m \u001b[38;5;124;03m        ignore_mismatched_sizes (`bool`, *optional*, defaults to `False`):\u001b[39;00m\n\u001b[0;32m   2115\u001b[0m \u001b[38;5;124;03m            Whether or not to raise an error if some of the weights from the checkpoint do not have the same size\u001b[39;00m\n\u001b[0;32m   2116\u001b[0m \u001b[38;5;124;03m            as the weights of the model (if for instance, you are instantiating a model with 10 labels from a\u001b[39;00m\n\u001b[0;32m   2117\u001b[0m \u001b[38;5;124;03m            checkpoint with 3 labels).\u001b[39;00m\n\u001b[0;32m   2118\u001b[0m \u001b[38;5;124;03m        force_download (`bool`, *optional*, defaults to `False`):\u001b[39;00m\n\u001b[0;32m   2119\u001b[0m \u001b[38;5;124;03m            Whether or not to force the (re-)download of the model weights and configuration files, overriding the\u001b[39;00m\n\u001b[0;32m   2120\u001b[0m \u001b[38;5;124;03m            cached versions if they exist.\u001b[39;00m\n\u001b[0;32m   2121\u001b[0m \u001b[38;5;124;03m        resume_download (`bool`, *optional*, defaults to `False`):\u001b[39;00m\n\u001b[0;32m   2122\u001b[0m \u001b[38;5;124;03m            Whether or not to delete incompletely received files. Will attempt to resume the download if such a\u001b[39;00m\n\u001b[0;32m   2123\u001b[0m \u001b[38;5;124;03m            file exists.\u001b[39;00m\n\u001b[0;32m   2124\u001b[0m \u001b[38;5;124;03m        proxies (`Dict[str, str]`, *optional*):\u001b[39;00m\n\u001b[0;32m   2125\u001b[0m \u001b[38;5;124;03m            A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\u001b[39;00m\n\u001b[0;32m   2126\u001b[0m \u001b[38;5;124;03m            'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\u001b[39;00m\n\u001b[0;32m   2127\u001b[0m \u001b[38;5;124;03m        output_loading_info(`bool`, *optional*, defaults to `False`):\u001b[39;00m\n\u001b[0;32m   2128\u001b[0m \u001b[38;5;124;03m            Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.\u001b[39;00m\n\u001b[0;32m   2129\u001b[0m \u001b[38;5;124;03m        local_files_only(`bool`, *optional*, defaults to `False`):\u001b[39;00m\n\u001b[0;32m   2130\u001b[0m \u001b[38;5;124;03m            Whether or not to only look at local files (i.e., do not try to download the model).\u001b[39;00m\n\u001b[0;32m   2131\u001b[0m \u001b[38;5;124;03m        token (`str` or `bool`, *optional*):\u001b[39;00m\n\u001b[0;32m   2132\u001b[0m \u001b[38;5;124;03m            The token to use as HTTP bearer authorization for remote files. If `True`, or not specified, will use\u001b[39;00m\n\u001b[0;32m   2133\u001b[0m \u001b[38;5;124;03m            the token generated when running `huggingface-cli login` (stored in `~/.huggingface`).\u001b[39;00m\n\u001b[0;32m   2134\u001b[0m \u001b[38;5;124;03m        revision (`str`, *optional*, defaults to `\"main\"`):\u001b[39;00m\n\u001b[0;32m   2135\u001b[0m \u001b[38;5;124;03m            The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\u001b[39;00m\n\u001b[0;32m   2136\u001b[0m \u001b[38;5;124;03m            git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any\u001b[39;00m\n\u001b[0;32m   2137\u001b[0m \u001b[38;5;124;03m            identifier allowed by git.\u001b[39;00m\n\u001b[0;32m   2138\u001b[0m \n\u001b[0;32m   2139\u001b[0m \u001b[38;5;124;03m            <Tip>\u001b[39;00m\n\u001b[0;32m   2140\u001b[0m \n\u001b[0;32m   2141\u001b[0m \u001b[38;5;124;03m            To test a pull request you made on the Hub, you can pass `revision=\"refs/pr/<pr_number>\".\u001b[39;00m\n\u001b[0;32m   2142\u001b[0m \n\u001b[0;32m   2143\u001b[0m \u001b[38;5;124;03m            </Tip>\u001b[39;00m\n\u001b[0;32m   2144\u001b[0m \n\u001b[0;32m   2145\u001b[0m \u001b[38;5;124;03m        mirror (`str`, *optional*):\u001b[39;00m\n\u001b[0;32m   2146\u001b[0m \u001b[38;5;124;03m            Mirror source to accelerate downloads in China. If you are from China and have an accessibility\u001b[39;00m\n\u001b[0;32m   2147\u001b[0m \u001b[38;5;124;03m            problem, you can set this option to resolve it. Note that we do not guarantee the timeliness or safety.\u001b[39;00m\n\u001b[0;32m   2148\u001b[0m \u001b[38;5;124;03m            Please refer to the mirror site for more information.\u001b[39;00m\n\u001b[0;32m   2149\u001b[0m \u001b[38;5;124;03m        _fast_init(`bool`, *optional*, defaults to `True`):\u001b[39;00m\n\u001b[0;32m   2150\u001b[0m \u001b[38;5;124;03m            Whether or not to disable fast initialization.\u001b[39;00m\n\u001b[0;32m   2151\u001b[0m \n\u001b[0;32m   2152\u001b[0m \u001b[38;5;124;03m            <Tip warning={true}>\u001b[39;00m\n\u001b[0;32m   2153\u001b[0m \n\u001b[0;32m   2154\u001b[0m \u001b[38;5;124;03m            One should only disable *_fast_init* to ensure backwards compatibility with `transformers.__version__ <\u001b[39;00m\n\u001b[0;32m   2155\u001b[0m \u001b[38;5;124;03m            4.6.0` for seeded model initialization. This argument will be removed at the next major version. See\u001b[39;00m\n\u001b[0;32m   2156\u001b[0m \u001b[38;5;124;03m            [pull request 11471](https://github.com/huggingface/transformers/pull/11471) for more information.\u001b[39;00m\n\u001b[0;32m   2157\u001b[0m \n\u001b[0;32m   2158\u001b[0m \u001b[38;5;124;03m            </Tip>\u001b[39;00m\n\u001b[0;32m   2159\u001b[0m \n\u001b[0;32m   2160\u001b[0m \u001b[38;5;124;03m        > Parameters for big model inference\u001b[39;00m\n\u001b[0;32m   2161\u001b[0m \n\u001b[0;32m   2162\u001b[0m \u001b[38;5;124;03m        low_cpu_mem_usage(`bool`, *optional*):\u001b[39;00m\n\u001b[0;32m   2163\u001b[0m \u001b[38;5;124;03m            Tries to not use more than 1x model size in CPU memory (including peak memory) while loading the model.\u001b[39;00m\n\u001b[0;32m   2164\u001b[0m \u001b[38;5;124;03m            This is an experimental feature and a subject to change at any moment.\u001b[39;00m\n\u001b[0;32m   2165\u001b[0m \u001b[38;5;124;03m        torch_dtype (`str` or `torch.dtype`, *optional*):\u001b[39;00m\n\u001b[0;32m   2166\u001b[0m \u001b[38;5;124;03m            Override the default `torch.dtype` and load the model under a specific `dtype`. The different options\u001b[39;00m\n\u001b[0;32m   2167\u001b[0m \u001b[38;5;124;03m            are:\u001b[39;00m\n\u001b[0;32m   2168\u001b[0m \n\u001b[0;32m   2169\u001b[0m \u001b[38;5;124;03m            1. `torch.float16` or `torch.bfloat16` or `torch.float`: load in a specified\u001b[39;00m\n\u001b[0;32m   2170\u001b[0m \u001b[38;5;124;03m              `dtype`, ignoring the model's `config.torch_dtype` if one exists. If not specified\u001b[39;00m\n\u001b[0;32m   2171\u001b[0m \u001b[38;5;124;03m              - the model will get loaded in `torch.float` (fp32).\u001b[39;00m\n\u001b[0;32m   2172\u001b[0m \n\u001b[0;32m   2173\u001b[0m \u001b[38;5;124;03m            2. `\"auto\"` - A `torch_dtype` entry in the `config.json` file of the model will be\u001b[39;00m\n\u001b[0;32m   2174\u001b[0m \u001b[38;5;124;03m              attempted to be used. If this entry isn't found then next check the `dtype` of the first weight in\u001b[39;00m\n\u001b[0;32m   2175\u001b[0m \u001b[38;5;124;03m              the checkpoint that's of a floating point type and use that as `dtype`. This will load the model\u001b[39;00m\n\u001b[0;32m   2176\u001b[0m \u001b[38;5;124;03m              using the `dtype` it was saved in at the end of the training. It can't be used as an indicator of how\u001b[39;00m\n\u001b[0;32m   2177\u001b[0m \u001b[38;5;124;03m              the model was trained. Since it could be trained in one of half precision dtypes, but saved in fp32.\u001b[39;00m\n\u001b[0;32m   2178\u001b[0m \n\u001b[0;32m   2179\u001b[0m \u001b[38;5;124;03m            <Tip>\u001b[39;00m\n\u001b[0;32m   2180\u001b[0m \n\u001b[0;32m   2181\u001b[0m \u001b[38;5;124;03m            For some models the `dtype` they were trained in is unknown - you may try to check the model's paper or\u001b[39;00m\n\u001b[0;32m   2182\u001b[0m \u001b[38;5;124;03m            reach out to the authors and ask them to add this information to the model's card and to insert the\u001b[39;00m\n\u001b[0;32m   2183\u001b[0m \u001b[38;5;124;03m            `torch_dtype` entry in `config.json` on the hub.\u001b[39;00m\n\u001b[1;32m-> 2184\u001b[0m \n\u001b[0;32m   2185\u001b[0m \u001b[38;5;124;03m            </Tip>\u001b[39;00m\n\u001b[0;32m   2186\u001b[0m \n\u001b[0;32m   2187\u001b[0m \u001b[38;5;124;03m        device_map (`str` or `Dict[str, Union[int, str, torch.device]]` or `int` or `torch.device`, *optional*):\u001b[39;00m\n\u001b[0;32m   2188\u001b[0m \u001b[38;5;124;03m            A map that specifies where each submodule should go. It doesn't need to be refined to each\u001b[39;00m\n\u001b[0;32m   2189\u001b[0m \u001b[38;5;124;03m            parameter/buffer name, once a given module name is inside, every submodule of it will be sent to the\u001b[39;00m\n\u001b[0;32m   2190\u001b[0m \u001b[38;5;124;03m            same device. If we only pass the device (*e.g.*, `\"cpu\"`, `\"cuda:1\"`, `\"mps\"`, or a GPU ordinal rank\u001b[39;00m\n\u001b[0;32m   2191\u001b[0m \u001b[38;5;124;03m            like `1`) on which the model will be allocated, the device map will map the entire model to this\u001b[39;00m\n\u001b[0;32m   2192\u001b[0m \u001b[38;5;124;03m            device. Passing `device_map = 0` means put the whole model on GPU 0.\u001b[39;00m\n\u001b[0;32m   2193\u001b[0m \n\u001b[0;32m   2194\u001b[0m \u001b[38;5;124;03m            To have Accelerate compute the most optimized `device_map` automatically, set `device_map=\"auto\"`. For\u001b[39;00m\n\u001b[0;32m   2195\u001b[0m \u001b[38;5;124;03m            more information about each option see [designing a device\u001b[39;00m\n\u001b[0;32m   2196\u001b[0m \u001b[38;5;124;03m            map](https://hf.co/docs/accelerate/main/en/usage_guides/big_modeling#designing-a-device-map).\u001b[39;00m\n\u001b[0;32m   2197\u001b[0m \u001b[38;5;124;03m        max_memory (`Dict`, *optional*):\u001b[39;00m\n\u001b[0;32m   2198\u001b[0m \u001b[38;5;124;03m            A dictionary device identifier to maximum memory. Will default to the maximum memory available for each\u001b[39;00m\n\u001b[0;32m   2199\u001b[0m \u001b[38;5;124;03m            GPU and the available CPU RAM if unset.\u001b[39;00m\n\u001b[0;32m   2200\u001b[0m \u001b[38;5;124;03m        offload_folder (`str` or `os.PathLike`, *optional*):\u001b[39;00m\n\u001b[0;32m   2201\u001b[0m \u001b[38;5;124;03m            If the `device_map` contains any value `\"disk\"`, the folder where we will offload weights.\u001b[39;00m\n\u001b[0;32m   2202\u001b[0m \u001b[38;5;124;03m        offload_state_dict (`bool`, *optional*):\u001b[39;00m\n\u001b[0;32m   2203\u001b[0m \u001b[38;5;124;03m            If `True`, will temporarily offload the CPU state dict to the hard drive to avoid getting out of CPU\u001b[39;00m\n\u001b[0;32m   2204\u001b[0m \u001b[38;5;124;03m            RAM if the weight of the CPU state dict + the biggest shard of the checkpoint does not fit. Defaults to\u001b[39;00m\n\u001b[0;32m   2205\u001b[0m \u001b[38;5;124;03m            `True` when there is some disk offload.\u001b[39;00m\n\u001b[0;32m   2206\u001b[0m \u001b[38;5;124;03m        load_in_8bit (`bool`, *optional*, defaults to `False`):\u001b[39;00m\n\u001b[0;32m   2207\u001b[0m \u001b[38;5;124;03m            If `True`, will convert the loaded model into mixed-8bit quantized model. To use this feature please\u001b[39;00m\n\u001b[0;32m   2208\u001b[0m \u001b[38;5;124;03m            install `bitsandbytes` (`pip install -U bitsandbytes`).\u001b[39;00m\n\u001b[0;32m   2209\u001b[0m \u001b[38;5;124;03m        load_in_4bit (`bool`, *optional*, defaults to `False`):\u001b[39;00m\n\u001b[0;32m   2210\u001b[0m \u001b[38;5;124;03m            If `True`, will convert the loaded model into 4bit precision quantized model. To use this feature\u001b[39;00m\n\u001b[0;32m   2211\u001b[0m \u001b[38;5;124;03m            install the latest version of `bitsandbytes` (`pip install -U bitsandbytes`).\u001b[39;00m\n\u001b[0;32m   2212\u001b[0m \u001b[38;5;124;03m        quantization_config (`Union[QuantizationConfigMixin,Dict]`, *optional*):\u001b[39;00m\n\u001b[0;32m   2213\u001b[0m \u001b[38;5;124;03m            A dictionary of configuration parameters or a QuantizationConfigMixin object for quantization (e.g\u001b[39;00m\n\u001b[0;32m   2214\u001b[0m \u001b[38;5;124;03m            bitsandbytes, gptq)\u001b[39;00m\n\u001b[0;32m   2215\u001b[0m \u001b[38;5;124;03m        subfolder (`str`, *optional*, defaults to `\"\"`):\u001b[39;00m\n\u001b[0;32m   2216\u001b[0m \u001b[38;5;124;03m            In case the relevant files are located inside a subfolder of the model repo on huggingface.co, you can\u001b[39;00m\n\u001b[0;32m   2217\u001b[0m \u001b[38;5;124;03m            specify the folder name here.\u001b[39;00m\n\u001b[0;32m   2218\u001b[0m \u001b[38;5;124;03m        variant (`str`, *optional*):\u001b[39;00m\n\u001b[0;32m   2219\u001b[0m \u001b[38;5;124;03m            If specified load weights from `variant` filename, *e.g.* pytorch_model.<variant>.bin. `variant` is\u001b[39;00m\n\u001b[0;32m   2220\u001b[0m \u001b[38;5;124;03m            ignored when using `from_tf` or `from_flax`.\u001b[39;00m\n\u001b[0;32m   2221\u001b[0m \u001b[38;5;124;03m        use_safetensors (`bool`, *optional*, defaults to `None`):\u001b[39;00m\n\u001b[0;32m   2222\u001b[0m \u001b[38;5;124;03m            Whether or not to use `safetensors` checkpoints. Defaults to `None`. If not specified and `safetensors`\u001b[39;00m\n\u001b[0;32m   2223\u001b[0m \u001b[38;5;124;03m            is not installed, it will be set to `False`.\u001b[39;00m\n\u001b[0;32m   2224\u001b[0m \n\u001b[0;32m   2225\u001b[0m \u001b[38;5;124;03m        kwargs (remaining dictionary of keyword arguments, *optional*):\u001b[39;00m\n\u001b[0;32m   2226\u001b[0m \u001b[38;5;124;03m            Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,\u001b[39;00m\n\u001b[0;32m   2227\u001b[0m \u001b[38;5;124;03m            `output_attentions=True`). Behaves differently depending on whether a `config` is provided or\u001b[39;00m\n\u001b[0;32m   2228\u001b[0m \u001b[38;5;124;03m            automatically loaded:\u001b[39;00m\n\u001b[0;32m   2229\u001b[0m \n\u001b[0;32m   2230\u001b[0m \u001b[38;5;124;03m                - If a configuration is provided with `config`, `**kwargs` will be directly passed to the\u001b[39;00m\n\u001b[0;32m   2231\u001b[0m \u001b[38;5;124;03m                  underlying model's `__init__` method (we assume all relevant updates to the configuration have\u001b[39;00m\n\u001b[0;32m   2232\u001b[0m \u001b[38;5;124;03m                  already been done)\u001b[39;00m\n\u001b[0;32m   2233\u001b[0m \u001b[38;5;124;03m                - If a configuration is not provided, `kwargs` will be first passed to the configuration class\u001b[39;00m\n\u001b[0;32m   2234\u001b[0m \u001b[38;5;124;03m                  initialization function ([`~PretrainedConfig.from_pretrained`]). Each key of `kwargs` that\u001b[39;00m\n\u001b[0;32m   2235\u001b[0m \u001b[38;5;124;03m                  corresponds to a configuration attribute will be used to override said attribute with the\u001b[39;00m\n\u001b[0;32m   2236\u001b[0m \u001b[38;5;124;03m                  supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute\u001b[39;00m\n\u001b[0;32m   2237\u001b[0m \u001b[38;5;124;03m                  will be passed to the underlying model's `__init__` function.\u001b[39;00m\n\u001b[0;32m   2238\u001b[0m \n\u001b[0;32m   2239\u001b[0m \u001b[38;5;124;03m    <Tip>\u001b[39;00m\n\u001b[0;32m   2240\u001b[0m \n\u001b[0;32m   2241\u001b[0m \u001b[38;5;124;03m    Activate the special [\"offline-mode\"](https://huggingface.co/transformers/installation.html#offline-mode) to\u001b[39;00m\n\u001b[0;32m   2242\u001b[0m \u001b[38;5;124;03m    use this method in a firewalled environment.\u001b[39;00m\n\u001b[0;32m   2243\u001b[0m \n\u001b[0;32m   2244\u001b[0m \u001b[38;5;124;03m    </Tip>\u001b[39;00m\n\u001b[0;32m   2245\u001b[0m \n\u001b[0;32m   2246\u001b[0m \u001b[38;5;124;03m    Examples:\u001b[39;00m\n\u001b[0;32m   2247\u001b[0m \n\u001b[0;32m   2248\u001b[0m \u001b[38;5;124;03m    ```python\u001b[39;00m\n\u001b[0;32m   2249\u001b[0m \u001b[38;5;124;03m    >>> from transformers import BertConfig, BertModel\u001b[39;00m\n\u001b[0;32m   2250\u001b[0m \n\u001b[0;32m   2251\u001b[0m \u001b[38;5;124;03m    >>> # Download model and configuration from huggingface.co and cache.\u001b[39;00m\n\u001b[0;32m   2252\u001b[0m \u001b[38;5;124;03m    >>> model = BertModel.from_pretrained(\"bert-base-uncased\")\u001b[39;00m\n\u001b[0;32m   2253\u001b[0m \u001b[38;5;124;03m    >>> # Model was saved using *save_pretrained('./test/saved_model/')* (for example purposes, not runnable).\u001b[39;00m\n\u001b[0;32m   2254\u001b[0m \u001b[38;5;124;03m    >>> model = BertModel.from_pretrained(\"./test/saved_model/\")\u001b[39;00m\n\u001b[0;32m   2255\u001b[0m \u001b[38;5;124;03m    >>> # Update configuration during loading.\u001b[39;00m\n\u001b[0;32m   2256\u001b[0m \u001b[38;5;124;03m    >>> model = BertModel.from_pretrained(\"bert-base-uncased\", output_attentions=True)\u001b[39;00m\n\u001b[0;32m   2257\u001b[0m \u001b[38;5;124;03m    >>> assert model.config.output_attentions == True\u001b[39;00m\n\u001b[0;32m   2258\u001b[0m \u001b[38;5;124;03m    >>> # Loading from a TF checkpoint file instead of a PyTorch model (slower, for example purposes, not runnable).\u001b[39;00m\n\u001b[0;32m   2259\u001b[0m \u001b[38;5;124;03m    >>> config = BertConfig.from_json_file(\"./tf_model/my_tf_model_config.json\")\u001b[39;00m\n\u001b[0;32m   2260\u001b[0m \u001b[38;5;124;03m    >>> model = BertModel.from_pretrained(\"./tf_model/my_tf_checkpoint.ckpt.index\", from_tf=True, config=config)\u001b[39;00m\n\u001b[0;32m   2261\u001b[0m \u001b[38;5;124;03m    >>> # Loading from a Flax checkpoint file instead of a PyTorch model (slower)\u001b[39;00m\n\u001b[0;32m   2262\u001b[0m \u001b[38;5;124;03m    >>> model = BertModel.from_pretrained(\"bert-base-uncased\", from_flax=True)\u001b[39;00m\n\u001b[0;32m   2263\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[0;32m   2264\u001b[0m \n\u001b[0;32m   2265\u001b[0m \u001b[38;5;124;03m    * `low_cpu_mem_usage` algorithm:\u001b[39;00m\n\u001b[0;32m   2266\u001b[0m \n\u001b[0;32m   2267\u001b[0m \u001b[38;5;124;03m    This is an experimental function that loads the model using ~1x model size CPU memory\u001b[39;00m\n\u001b[0;32m   2268\u001b[0m \n\u001b[0;32m   2269\u001b[0m \u001b[38;5;124;03m    Here is how it works:\u001b[39;00m\n\u001b[0;32m   2270\u001b[0m \n\u001b[0;32m   2271\u001b[0m \u001b[38;5;124;03m    1. save which state_dict keys we have\u001b[39;00m\n\u001b[0;32m   2272\u001b[0m \u001b[38;5;124;03m    2. drop state_dict before the model is created, since the latter takes 1x model size CPU memory\u001b[39;00m\n\u001b[0;32m   2273\u001b[0m \u001b[38;5;124;03m    3. after the model has been instantiated switch to the meta device all params/buffers that\u001b[39;00m\n\u001b[0;32m   2274\u001b[0m \u001b[38;5;124;03m    are going to be replaced from the loaded state_dict\u001b[39;00m\n\u001b[0;32m   2275\u001b[0m \u001b[38;5;124;03m    4. load state_dict 2nd time\u001b[39;00m\n\u001b[0;32m   2276\u001b[0m \u001b[38;5;124;03m    5. replace the params/buffers from the state_dict\u001b[39;00m\n\u001b[0;32m   2277\u001b[0m \n\u001b[0;32m   2278\u001b[0m \u001b[38;5;124;03m    Currently, it can't handle deepspeed ZeRO stage 3 and ignores loading errors\u001b[39;00m\n\u001b[0;32m   2279\u001b[0m \n\u001b[0;32m   2280\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   2281\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m   2282\u001b[0m     from_tf \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_tf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\modeling_utils.py:386\u001b[0m, in \u001b[0;36mload_state_dict\u001b[1;34m(checkpoint_file)\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_sharded_checkpoint\u001b[39m(model, folder, strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, prefer_safe\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    369\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;124;03m    This is the same as\u001b[39;00m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;124;03m    [`torch.nn.Module.load_state_dict`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=load_state_dict#torch.nn.Module.load_state_dict)\u001b[39;00m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;124;03m    but for a sharded checkpoint.\u001b[39;00m\n\u001b[0;32m    373\u001b[0m \n\u001b[0;32m    374\u001b[0m \u001b[38;5;124;03m    This load is performed efficiently: each checkpoint shard is loaded one by one in RAM and deleted after being\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;124;03m    loaded in the model.\u001b[39;00m\n\u001b[0;32m    376\u001b[0m \n\u001b[0;32m    377\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    378\u001b[0m \u001b[38;5;124;03m        model (`torch.nn.Module`): The model in which to load the checkpoint.\u001b[39;00m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;124;03m        folder (`str` or `os.PathLike`): A path to a folder containing the sharded checkpoint.\u001b[39;00m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;124;03m        strict (`bool`, *optional`, defaults to `True`):\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;124;03m            Whether to strictly enforce that the keys in the model state dict match the keys in the sharded checkpoint.\u001b[39;00m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;124;03m        prefer_safe (`bool`, *optional*, defaults to `False`)\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;124;03m            If both safetensors and PyTorch save files are present in checkpoint and `prefer_safe` is True, the\u001b[39;00m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;124;03m            safetensors files will be loaded. Otherwise, PyTorch files are always loaded when possible.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \n\u001b[1;32m--> 386\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;124;03m        `NamedTuple`: A named tuple with `missing_keys` and `unexpected_keys` fields\u001b[39;00m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;124;03m            - `missing_keys` is a list of str containing the missing keys\u001b[39;00m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;124;03m            - `unexpected_keys` is a list of str containing the unexpected keys\u001b[39;00m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    391\u001b[0m     \u001b[38;5;66;03m# Load the index\u001b[39;00m\n\u001b[0;32m    392\u001b[0m     index_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder, WEIGHTS_INDEX_NAME)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'safe_open' is not defined"
     ]
    }
   ],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "reader = PdfReader(\"Receipt.pdf\")\n",
    "pdf_text = \"\"\n",
    "print(\"Total pages=\",len(reader.pages)) \n",
    "page_numbers_to_read = [0] # Specify page number\n",
    "\n",
    "# Read the given pages\n",
    "for page in page_numbers_to_read:\n",
    "    page_text = reader.pages[page].extract_text()\n",
    "    pdf_text += page_text\n",
    "\n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-large\").to(device)\n",
    "\n",
    "def query_from_list(query, options, tok_len):\n",
    "    t5query = f\"\"\"Question: \"{query}\" Context: {options}\"\"\"\n",
    "    inputs = tokenizer(t5query, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=tok_len)\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "questions = [\n",
    "    \"What is the Invoice no?\",\n",
    "    \"What is the Invoice issue Date?\",\n",
    "    \"What is the Seller name?\",\n",
    "    \"What is the Client name?\",\n",
    "    \"What is the Client Address?\",\n",
    "    \"What is the Seller Address?\",\n",
    "    \"What is the Total Net worth?\",\n",
    "    \"What is the total Growth worth amount?\"\n",
    "]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5690dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the LLM with input data and instruction\n",
    "input_data=pdf_text\n",
    "results = {question:(query_from_list(question,  input_data+\"\", 30))  for question in questions}\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
